# DeepCubeA 实现细节

## 1. 魔方状态表示

魔方共有54个小面，6种颜色。我们为每个小面分配唯一编号，由其所在大面和在大面上的位置决定。在复原状态下，大面的位置和颜色关系固定为：

- U面(上)：白色
- F面(前)：绿色
- L面(左)：橙色
- R面(右)：红色
- B面(后)：蓝色
- D面(下)：黄色

我们使用54维向量表示魔方状态，每个元素表示对应小面的颜色。例如，复原状态的向量表示为：

```python
# 用整数表示颜色（0=白, 1=红, 2=绿, 3=黄, 4=橙, 5=蓝）
initial_state = [
    0,0,0, 0,0,0, 0,0,0,  # U面
    1,1,1, 1,1,1, 1,1,1,  # R面
    2,2,2, 2,2,2, 2,2,2,  # F面
    3,3,3, 3,3,3, 3,3,3,  # D面
    4,4,4, 4,4,4, 4,4,4,  # L面
    5,5,5, 5,5,5, 5,5,5   # B面
]
```

输入DNN模型时，状态向量会转换为54×6的one-hot编码。

## 2. 动作表示

使用面表示法（face notation）表示魔方旋转动作：

- F、B、L、R、U、D：分别表示旋转前、后、左、右、上、下六个面
- 单个字母表示顺时针旋转90°，字母后加撇号(')表示逆时针旋转90°

例如：
- R：右面顺时针旋转90°
- R'：右面逆时针旋转90°

## 3. 深度近似值迭代（Deep Approximate Value Iteration）

我们使用DNN网络近似表示状态到目标状态的距离（cost-to-go）J(s)。训练采用逐步训练的方式，模拟值迭代过程：

1. 在第K次训练时，我们从原始魔方状态随机打乱1-K次生成训练样本
2. 利用已训练好的第K-1次模型J_{θ_{K-1}}提供监督信号
3. 通过公式J_{θ_K}(s) = min_a [J_{θ_{K-1}}(A(s,a)) + 1]更新当前模型

## 4. 训练伪代码

```python
# Algorithm 1: DAVI
# 输入:
#   B: 批次大小
#   K: 最大打乱次数
#   M: 训练迭代次数
#   C: 收敛检查频率
#   ε: 误差阈值
# 输出:
#   θ: 训练好的神经网络参数

theta = initialize_parameters()
theta_e = theta

for m in 1 to M:
    X = get_scrambled_states(B, K)
    for x_i in X:
        y_i = min_a [g_theta_e(A(x_i, a)) + 1]
    theta = train(X, y, theta)
    if (m mod C == 0) and (loss < ε):
        theta_e = theta

eturn theta
```

## 5. 推理：BWAS搜索算法

使用A*搜索算法，维护两个状态集合OPEN和CLOSED：

- OPEN：待扩展状态
- CLOSED：已扩展状态

路径长度计算公式：f(x) = λ*g(x) + h(x)

- h(x)：启发函数，用训练好的DNN模型J_θ表示
- g(x)：初始状态到状态x的实际路径长度
- λ：权重参数，原文中设置为0.6

每次从OPEN集合中选择路径最短的N个状态（原文中N=10000）加入CLOSED集合，并扩展其邻居状态。

## 6. 神经网络架构与实现细节

### 模型架构

1. **隐藏层**：前两个隐藏层大小分别为5000和1000，全连接
2. **残差块**：4个残差块，每个包含两个大小为1000的隐藏层
3. **输出层**：单个线性单元，表示cost-to-go估计
4. **归一化和激活**：所有隐藏层使用批归一化和ReLU激活函数

### 训练细节

1. **批次大小**：10,000
2. **优化器**：ADAM
3. **正则化**：无
4. **最大打乱次数(K)**：30
5. **误差阈值(ε)**：0.20（模型无法收敛到原文中的0.05）
6. **收敛检查**：每5,000次迭代检查一次
7. **每epoch迭代次数**：1000次
8. **训练时间**：约2小时
9. **设备**：单个 NVIDIA A100
10. **显存需求**：约 4GB

## Debug

1. DNN中的所乘的 K 可以去掉。
2. 还是用MSE_LOSS感觉比较好一点，先别用relative了感觉❌。还是要用relative，因为如果不用，K=2的时候模型收敛位置的损失就高达0.15，收敛判别的依据感觉并不准确。
3. 不要在dataset中一个数据一个数据放到torch上，这样效率太低，最好还是在training_loop中成批量一起转移。
4. 有一个问题，如果说model_theta_e遇到一个自己没见过的状态，实际上的距离是K+1，但是它给出来了一个小于K的值，然后这个时候模型就会低估当前这个状态的距离？在实际的训练结果中，模型的输出确实也相对比较低，比如30次打乱给出的最高分才是12multiple，这种低估不知道是否有问题。
5. 还是因为粗心Cube的旋转操作出错了。
